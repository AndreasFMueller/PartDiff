%
% pdgl2ord.tex -- Klassifikation linearer PDGL zweiter Ordnung
%
% (c) 2016 Prof Dr Andreas Mueller, Hochschule Rapperswil
%
\chapter{Partielle Differentialgleichungen zweiter Ordnung\label{chapter-2ordnung}}
\lhead{Lineare PDGL 2. Ordnung}
Lineare Differentialgleichungen zweiter Ordnung haben die Form
\begin{equation}
\sum_{i,j=1}^na_{ij}\frac{\partial^2}{\partial x_i\,\partial x_j} u
+
\sum_{i=1}^nb_i\frac{\partial}{\partial x_i} u+cu=f.
\label{operator2ordnung}
\end{equation}
Alle Musterbeispiele im Kapitel \ref{chapter-beispiele} passen auf dieses 
Beispiel.
Trotz der oberflächlichen Gemeinsamkeit zeigen die Lösungen
der Beispielgleichungen grundsätzlich verschiedenes Verhalten.
\begin{itemize}
\item Die Wellengleichung hat Lösungen, die sich mit endlicher
Ausbreitungsgeschwindigkeit im Definitionsgebiet ausbreiten. Eine Änderung
der Anfangsbedingung macht sich erst nach einer gewissen Zeit in
entfernten Teilen des Gebietes bemerkbar.
\item Änderungen der Randbedingungen der Wärmeleitungsgleichung wirken
sich sofort auf die Werte der Lösung zu späterer Zeit aus und breiten
sich mit unendlicher Geschwindigkeit durch das Gebiet aus, sie haben jedoch
keine Auswirkungen auf die Vergangenheit.
\item Ändert man die Randbedingung der Poisson-Gleichung $\Delta \varphi=f$,
ändert sich die Lösung sofort und überall im Gebiet.
\end{itemize}
In diesem Kapitel sollen lineare partielle Differentialgleichungen zweiter
Ordnung klassifiziert werden, und es soll gezeigt werden, dass alle
linearen partiellen Differentialgleichungen zweiter Ordnung im Wesentlichen
in eine der drei eben genannten Kategorien fallen.

\section{Standardform}
\rhead{Standardform}
In der Form (\ref{operator2ordnung}) wird die Differentialgleichung durch
die Grössen $(a_{ij})$, $b_i$ und $c$ bestimmt, die auch Funktionen
der unabhängigen Variablen $x_i$ sein können.
Allerdings ist nicht jeder Koeffizient gleich bedeutend, schliesslich muss
man davon ausgehen, dass eine Änderung der Koordinaten auch die Koeffizienten
$a_{ij}$ und $b_i$ verändern wird.
In diesem Abschnitt sollen daher die unter Koordinatentransformationen
unveränderlichen Eigenschaften der Koeffizienten ermittelt werden,
die für eine Klassifikation verwendet werden können.

\subsection{Lineare Transformationen der Koordinaten
\label{lineare-transformation}}
Eine Koordinatentransformation
\[
x_i=\sum_{j}t_{ij}x_j'
\]
transformiert die Ableitungen nach der Kettenregel
\[
\frac{\partial u}{\partial x'_j}
=
\sum_{i=1}^n
\frac{\partial x_i}{\partial x'_j} \frac{\partial u}{\partial x_i}
=
\sum_{i=1}^nt_{ij}\frac{\partial u}{\partial x_i}.
\]
Daraus lässt sich bereits ableiten, wie die Koeffizienten $b_i$ in der
Gleichung transformiert werden müssen.
Wir bezeichnen die Koeffizienten der ersten Ableitungen in
(\ref{operator2ordnung}) ausgedrückt in $x_i'$-Koordinaten mit
$b_i'$.
In beiden Koordinatensystemen muss der Term mit ersten Ableitungen gleich
sein, es muss also
\[
\sum_{i=1}^n b_i\frac{\partial u}{\partial x_i}
=
\sum_{i=1}^n b_i'\frac{\partial u}{\partial x_i'}
=
\sum_{i,j=1}^n b_i't_{ji}\frac{\partial u}{\partial x_j}
=
\sum_{j,i=1}^n b_j't_{ij}\frac{\partial u}{\partial x_i}
\]
gelten.
Dies kann aber nur gelten, wenn
\[
b_i = \sum_{j=1}^n t_{ij}b_j'.
\]
Bezeichnen wir die zur Matrix $(t_{ij})$ inverse Matrix mit $(t'_{ij})$,
dann können wir $b_i'$ durch $b_i$ 
\[
b_j'=\sum_{i=1}^n t'_{ji}b_i.
\]
Bezeichnen wir die Matrix mit Koeffizienten $t_{ij}$ als $T$ und den 
Vektor mit Koeffizienten $b_i$ mit $b$, und analog für $t_{ij}'$ und
$b_i'$, dann können wir die Koordinatentransformation in Matrixform
\[
b=Tb'
\qquad\Rightarrow\qquad
b'=T^{-1}b=T'b
\]
schreiben.

Analog können auch die zweiten Ableitungen behandelt werden.
Zunächst gilt für die Ableitungen:
\[
\frac{\partial}{\partial x_i'}\frac{\partial}{\partial x_j'} u
=
\frac{\partial}{\partial x_i'}
\sum_{k=1}^nt_{kj}\frac{\partial}{\partial x_k}u
=
\sum_{k,l=1}^nt_{li}t_{kj}\frac{\partial}{\partial x_l}\frac{\partial}{\partial x_k}u.
\]
Die transformierten Koordinaten $a'_{ij}$ müssen den gleichen Ausdruck
zweiter Ordnung ergeben, es muss also
\begin{align*}
\sum_{i,j=1}^n
a'_{ij}\frac{\partial}{\partial x_i'}\frac{\partial}{\partial x_j'} u
&=
\sum_{i,j=1}^n
a_{ij}'
\sum_{k,l=1}^n
t_{li}t_{kj}\frac{\partial}{\partial x_l}\frac{\partial}{\partial x_k}u.
=
\sum_{k,l=1}^n
\biggl(
\sum_{i,j=1}^n
a_{ij}'
t_{li}t_{kj}
\biggr)
\frac{\partial}{\partial x_l}\frac{\partial}{\partial x_k}u
\\
&=
\sum_{i,j=1}^n
\underbrace{
\biggl(
\sum_{k,l=1}^n
a_{lk}'
t_{il}t_{jk}
\biggr)}_{\textstyle a_{ij}}
\frac{\partial}{\partial x_i}\frac{\partial}{\partial x_j}u
\end{align*}
gelten.
Die Koeffizienten $a_{ij}$ müssen also nach der Regel
\[
a_{ij}=\sum_{k,l=1}^n t_{il}a_{lk}'t_{jk}
\]
transformiert werden, oder in Matrixform
\[
A=TA'T^t
\qquad\Rightarrow\qquad
A'=T'AT'^t.
\]
Diese Regeln gelten natürlich nur, wenn die Grössen $a_{ij}$ und $b_i$
Konstanten sind.
Andernfalls treten zusätzlich Terme auf, die von den Ableitungen der
Koeffizienten herrühren.

\subsection{Einfluss der Terme erster Ordnung
\label{einfluss-terme-erster-ordnung}}
Wir wollen zeigen, dass für die Frage, welche Randwerte wo einen
Einfluss auf die Lösung haben, für die Koeffizienten $b_i$ nicht
von Bedeutung sind.

Bei einer gewöhnlichen Differentialgleichung zweiter Ordnung beschreibt
der Koeffizient der ersten Ableitung die Dämpfung, die zu exponentieller
Abnahme der Amplitude führt.
Indem man die Lösung mit Hilfe eines Exponentialfaktors wieder verstärkt,
kann man die Dämpfung zum Verschwinden bringen, und klarer machen,
ob Schwingungslösungen oder exponentiell abfallende Lösungen vorliegen.

Dasselbe gilt für die Lösung $u(x)$ der Differentialgleichung
(\ref{operator2ordnung}).
Wir betrachen die Funktion
\[
v(x)=e^{k\cdot x} u(x),
\qquad
k\cdot x = \sum_{i=1}^n k_ix_i \quad(\text{Skalarprodukt})
\]
und suchen eine partielle Differentialgleichung, die $v$ genau dann löst,
wenn $u$ die Differentialgleichung
(\ref{operator2ordnung}) löst.

Dazu berechnen wir die Wirkung des Differentialoperators, der nur 
aus den zweiten Ableitungen besteht:
\[
M=\sum_{i,j=1}^n a_{ij}\frac{\partial^2}{\partial x_i\,\partial x_j}.
\]
Seine Wirkung auf der Funktion $v$ ist
\begin{align*}
Mv
&=
\sum_{i,j=1}^na_{ij}
\frac{\partial}{\partial x_i}
\frac{\partial}{\partial x_j} e^{k\cdot x}u
\\
&=
e^{k\cdot x}
\biggl(
\sum_{i,j=1}^na_{ij}
\frac{\partial^2 u}{\partial x_i\,\partial x_j}
+
\sum_{i,j=1}^n a_{ij}k_i\frac{\partial u}{\partial x_j}
+
\sum_{i,j=1}^n a_{ij}k_j\frac{\partial u}{\partial x_i}
+
\sum_{i,j=1}^n a_{ij}k_ik_ju
\biggr)
\\
&=
e^{k\cdot x}
\biggl(
\sum_{i,j=1}^na_{ij}
\frac{\partial^2 u}{\partial x_i\,\partial x_j}
+
2\sum_{i,j=1}^n a_{ij}k_i\frac{\partial u}{\partial x_j}
+
\sum_{i,j=1}^n a_{ij}k_ik_ju
\biggr)
\end{align*}
Im letzten Schritt haben wir verwendet, dass wir die Laufvariablen
in der zweiten Summe beliebig umbenennen können.

Jetzt nutzen wir die Möglichkeit, die Koeffizienten $k_i$ für unsere
Zwecke geeignet zu wählen.
Wir versuchen zu erreichen, dass der mittlere Term genau dem Term 
der ersten Ableitungen in (\ref{operator2ordnung}) entspricht.
Dazu muss gelten
\begin{equation}
b_j=2\sum_{i=1}^n a_{ij}k_i
\label{bequation}
\end{equation}
Falls die Matrix $A$ der Koeffizienten $a_{ij}$ regulär ist, lässt
sich dieses lineare Gleichungssystem lösen, es gilt
\[
k=
{A^t}^{-1}
\frac{b}2.
\]
Schreiben wir ausserdem
\[
\alpha = \sum_{i,j=1}^n a_{ij}k_ik_j,
\]
dann folgt, dass die Gleichung $Lu=f$ genau dann erfüllt ist, wenn
\begin{align}
Mv
&=
e^{k\cdot x}\biggl(
\sum_{i,j=1}^n a_{ij}\frac{\partial^2}{\partial x_i\,\partial x_j}u
+\sum_{i=1}^n b_i\frac{\partial u}{\partial x_i}+\alpha u
\biggr)
\\
&=
e^{k\cdot x}
(L+\alpha -c)u
=
e^{k\cdot x}Lu + (\alpha -c)v
\notag
\\
(M-(\alpha - c))v&=e^{k\cdot x}Lu=e^{k\cdot x}f.
\label{reducedoperator}
\end{align}
Die Gleichung (\ref{operator2ordnung}) wird also genau dann von der
Funktion $u$ gelöst, wenn die Funktion $v(x)=e^{k\cdot x}u(x)$ 
die Gleichung (\ref{reducedoperator}) löst.
Auch wird die Lösung $u$ der Gleichung (\ref{operator2ordnung})
im Punkt $x$ 
genau dann durch eine Änderung der Randbedingung verändert, 
wenn die gleiche Änderung der Randbedingung die Lösung $v$ 
der Gleichung (\ref{reducedoperator}) im selben Punkt verändert.

Zusammenfassend können wir daher sagen, dass falls die Matrix der
Koeffizienten $a_{ij}$ regulär ist, oder mindestens das Gleichungssystem
(\ref{bequation}) lösbar ist, für die Beantwortung der Anfangs
dieses Kapitels gestellten Fragen die Terme der ersten Ableitungen
ignoriert werden können.
Falls $a_{ij}$ nicht regulär ist, dann lassen sich die Terme der
ersten Ableitungen nicht alle wegtransformieren.

\subsection{Symbolmatrix}
Da die zweiten Ableitungen untereinandern vertauschen, also 
\[
\frac{\partial^2}{\partial x_i\,\partial x_j}
=
\frac{\partial^2}{\partial x_j\,\partial x_i},
\]
sind die Koeffizienten $a_{ij}$ und $a_{ji}$ voneinander abhängig.
Insbesondere kann man zusätzlich verlangen, dass $a_{ij}=a_{ji}$,
dass also die Matrix $A$ symmetrisch ist.

\begin{definition}Ist $L$ ein linearer Differentialoperator zweiter Ordnung
\[
Lu=\sum_{i,j=1}^na_{ij}\frac{\partial u}{\partial x_i\,\partial x_j},
\]
dann heisst die symmetrische Matrix
$(a_{ij})$ das Symbol des Operators.
\end{definition}

Im Abschnitt~\ref{einfluss-terme-erster-ordnung} haben wir gesehen,
dass die Terme mit ersten Ableitungen wegtransformiert werden können,
wenn die Gleichung 
\[
Ak=\frac{b}2
\]
lösbar ist.
Wenn $A$ nicht regulär ist, dann kann $b$ nur dann wegtransformiert werden,
wenn $b$ im Bild der Matrix $A$ liegt.
Vektoren senkrecht zum Bild können dagegen nicht wegtransformiert werden.
Falls also $b\perp \operatorname{im} A$, dann ist
\[
b\cdot Ax=(A^tb)\cdot x\;\forall x
\qquad\Rightarrow\qquad
Ab=0,
\]
ein Vektor $b$ im Kern von $A$ kann also nicht wegtransformiert werden.

\begin{beispiel}
Als Beispiel betrachten wir den Operator 
\[
L
=
\sum_{i=2}^n\frac{\partial^2}{\partial x_i^2}
+
\sum_{i=1}^nb_i\frac{\partial}{\partial x_i}
\]
mit der Symbolmatrix
\[
A=\begin{pmatrix}
      0&      0& \dots&0\\
      0&      1& \dots&0\\
\vdots &\vdots &\ddots&\vdots\\
      0&      0&\dots &1
\end{pmatrix}
\]
Der Vektor $e_1$ ist im Kern von $A$, also lässt sich die erste
Ableitung nach $x_1$ nicht wegtransformieren, die einfachste
Form, die wir für diesen Operator erreichen können ist daher
\[
\sum_{i=2}^n\frac{\partial^2}{\partial x_i^2}
+
\frac{\partial}{\partial x_1}.
\]
Dies ist der Operator, den wir für die Wärmeleitungsgleichung
kennengelernt haben.
\end{beispiel}

\section{Typen von Differentialgleichungen zweiter Ordnung}
\rhead{Klassifikation}
Im Abschnitt~\ref{lineare-transformation} wurde gezeigt, dass die Symbolmatrix
$A$ bei einer Koordinatentransformation mit der Matrix $T$ gemäss
\[
A'=TAT^t 
\]
transformiert wird.
In der linearen Algebra lernt man, dass man zu einer symmetrischen
Matrix $A$ immer eine orthogonale Matrix $T$ finden kann, so dass
$A'=TAT^t$ Diagonalform hat, wobei die Diagonalelemente die
Eigenwerte der Eigenvektoren sind.
\[
TAT^t
=
\begin{pmatrix}\lambda_1&\dots&0\\
\vdots&\ddots&\vdots\\
0&\dots&\lambda_n
\end{pmatrix}
\]
Man kann also durch geeignete Wahl des Koordinatensystems mindestens in
einem Punkt einen Differentialoperator zweiter Ordnung immer in die
Form
\[
\sum_{i=1}\lambda_i\frac{\partial^2}{\partial x_i^2}u
\]
bringen.

Durch eine Streckung der Koordinatenachse $x_i$ um $\sqrt{|\lambda_i|}$
kann man ausserdem erreichen, dass der Differentialoperator die
Form
\[
\sum_{i=1}\varepsilon_i\frac{\partial^2}{\partial x_i^2}u
\]
erhält, wobei die Zahlen $\varepsilon_i$ die Werte, $1$, $-1$ oder $0$
haben können.

Ausserdem können wir das Vorzeichen der ganzen Differentialgleichung
umkehren, dadurch ändert die Symbolmatrix das Vorzeichen, und die
Vorzeichen der Eigenwerte kehren um.
Man kann daher immer erreichen, dass die Mehrheit der nicht
verschwindenden Eigenwerte positiv sind.

Differentialoperatoren zweiter Ordnung werden also charakterisiert durch
die Vorzeichen der Eigenwerte der Symbolmatrix. Wir bezeichnen die Anzahl
der positiven Eigenwerte mit $P$, die Anzahl der negativen Eigenwerte
mit $N$, und die Anzahl der verschwindenden Eigenwerte mit $Z$.
Die Beispiele des Kapitels \ref{chapter-beispiele} können wie
folgt kategorisiert werden:
\begin{center}
\begin{tabular}{l|ccc}
Differentialoperator&P&N&Z
\\
\hline
Laplace&
$n$&$0$&$0$
\\
Wellengleichung&
$n-1$&$1$&$0$
\\
Wärmeleitung&
$n-1$&$0$&$1$
\end{tabular}
\end{center}
Daraus leiten wir die folgende Klassifikation ab:

\begin{definition} Die Differentialgleichung 
(\ref{operator2ordnung})
mit den Kennzahlen $P$, $N$ und $Z$ heissen
\begin{center}
\begin{tabular}{lcl}
hyperbolisch&falls&$Z=0$ und $P=1$ oder $P=n-1$\\
parabolisch&falls&$Z>0$\\
elliptisch&falls&$Z=0$ und $P=n$ oder $P=0$\\
ultrahyperbolisch&falls&$Z=0$ und $1<P<n-1$
\end{tabular}
\end{center}
\end{definition}
Insbesondere sind die Gleichungen des Kapitels \ref{chapter-beispiele}
Beispiele für diese Typen von partiellen Differentialgleichungen
wie folgt:
\begin{itemize}
\item {\bf elliptisch:} Potential
\item {\bf hyperbolisch:} Wellengleichung, linearisierte Überschallströmung
\item {\bf parabolisch:} Wärmeleitung, Diffusion
\end{itemize}

\section{Kanonische Form}
\rhead{Kanonische Form}
Für lineare PDGL mit konstanten Koeffizienten, für die also die Grössen
$a_{ij}$ nicht von den Koordinaten abhängen, kann man durch eine Drehung
des Koordinatensystems immer erreichen, dass die Koeffizentenmatrix diagonal
ist mit Diagonalelementen aus $\{0,\pm1\}$. Es stellt sich die Frage, ob
dies wohl auch bei partiellen Differentialgleichungen mit allgemeinen
Koeffizienten möglich ist.

In zwei Dimensionen lässt sich ein geeignetes Koordinatensystem mit folgender
Idee konstruieren. 
In jedem Punkt $(x,y)$ gibt es zwei Vektoren $\vec e_+$ und
$\vec e_-$, welche Eigenvektoren zum grösseren bzw.~kleineren Eigenvektor
der Matrix $A$ sind. Solange die Eigenwerte verschieden sind, lassen sich
die Vektorfelder $\vec e_+$ und $\vec e_-$ auf stetige Weise konstruieren.
Die Vektoren stehen in jedem Punkt senkrecht aufeinander.

Nun kann man zu jedem Vektorfeld eine Schar von Lösungskurven
konstruieren. Jede Schar hängt von einem Parameter ab, wir nennen die
beiden Parameter $\xi$ und $\eta$. Ein Punkt in der Ebene kann mit
den Koordinaten $\xi$ und $\eta$ beschrieben werden.
In diesen Koordinaten hat die Matrix $A$ Diagonalform.

In drei Dimensionen lässt sich diese Idee nicht mehr durchführen,
die Felder der Eigenvektoren müssen hier zusätzliche Bedingungen
erfüllen.

Da man aber in der Nähe eines Punktes immer erreichen kann, dass $A$ diagonal
ist, ist dort die Differentialgleichung näherungsweise diagonal.
Wenn wir vor allem an den allgemeinen Eigenschaften einer Lösung interessiert
sind, genügt es also
also für alle Eigenschaften, die sich auf das Verhalten der Lösung im kleinen
beziehen, die Standarddifferentialgleichungen des folgenden Katalogs
zu studieren:
\begin{align*}
\Delta u&=0\\
\partial_tu&=k\Delta u\\
\partial_t^2u&=a^2\Delta u\\
\end{align*}
was wir in dieser Reihenfolge in den folgenden drei Kapiteln tun
werden.

\section{Aufteilung der Lösung}
\rhead{Aufteilung der Lösung}
Wir möchten den Lösungsraum einer linearen Differentialgleichung
zweiter Ordnung mit dem Operator $L$ besser verstehen.
Wir gehen daher aus von dem Problem
\begin{equation}
\begin{aligned}
Lu&=f&&\qquad\text{in $\Omega$}\\
 u&=g&&\qquad\text{auf $\partial\Omega$,}
\end{aligned}
\label{pdgl2ord:allg}
\end{equation}
und suchen eine Lösung.
Da die Gleichung linear ist, können wir die Lösung als Linearkombination
von Funktionen aufbauen, welche das Problem teilweise lösen.

Als erstes können wir versuchen, eine Lösung der Differentialgleichung
ohne Berücksichtigung der Randbedingungen zu lösen.
Nennen wir die Lösung $u_p$, dann gilt
\begin{equation}
Lu_p=f\qquad\text{in $\Omega$.}
\label{pdgl2ord:up}
\end{equation}
Über die Werte von $u_p$ auf dem Rand $\partial\Omega$ von $\Omega$
setzen wir nichts voraus.

Um das ursprüngliche Problem zu lösen, brauchen wir also noch einen
zweiten Summanden $u_r$. 
Damit $u_p+u_r$ uns der Lösung des Problems näher bringt, muss in
$\Omega$ immer noch die Differentialgleichung gelten:
\[
L(u_p+u_r)=f+Lu_r=f\qquad\Rightarrow\qquad Lu_r=0.
\qquad\text{in $\Omega$.}
\]
Ausserdem wollen wir erreichen, dass die Summe die Randbedingung erfüllt
\[
u_p+u_r=g\qquad\Rightarrow\qquad u_r=g-u_p\qquad\text{auf $\partial\Omega$.}
\]
Die Funktion $u_r$ muss also das folgende Problem lösen:
\begin{equation}
\begin{aligned}
Lu_r&=0&&\qquad\text{in $\Omega$}\\
 u_r&=g-u_p&&\qquad\text{auf $\partial\Omega$}
\end{aligned}
\label{pdgl2ord:ur}
\end{equation}
Damit haben wir in $u_p+u_r$ eine Lösung gefunden.

Es könnte aber immer noch sein, dass das ursprüngliche Problem weitere
Lösungen hat.
Solche weiteren Lösungen müssen aus $u_p+u_r$ durch hinzufügen eines
weiteren Summanden $u_h$ zu bekommen sein, der aber die Differentialgleichung
im Inneren und die Randbedingungen nicht zerstören darf:
\begin{equation}
\begin{aligned}
L(u_p+u_r+u_h)&=f+Lu_h&&\Rightarrow&Lu_h&=0&&\text{in $\Omega$}\\
  u_p+u_r+u_h &=g +u_h&&\Rightarrow& u_h&=0&&\text{auf $\partial\Omega$}
\end{aligned}
\label{pdgl2ord:uh}
\end{equation}
Die Lösung des Problems~(\ref{pdgl2ord:allg}) setzt sich also aus
drei Summanden $u_p$, $u_r$ und $u_h$ zusammen:
\begin{enumerate}
\item $u_p$ erfüllt die Differentialgleichung~(\ref{pdgl2ord:up}), ohne
Berücksichtigung irgendwelcher Randbedingungen.
\item $u_r$ löst das homogene Problem~(\ref{pdgl2ord:ur}), die Summe
$u_p+u_r$ löst das ursprüngliche Problem.
\item Wenn das homogene Problem~(\ref{pdgl2ord:uh}) mit homogenen
Randbedingungen nichttriviale Lösungen hat, dann ist die Lösung
des ursprünglichen Problems nicht eindeutig.
\end{enumerate}


